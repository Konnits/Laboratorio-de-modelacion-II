{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura y preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data.xlsb\")\n",
    "data[\"Fecha_Reporte\"] = data[\"Fecha_Reporte\"].apply(lambda x : pd.to_datetime(\"1899-12-30\") + datetime.timedelta(days=x))\n",
    "data[\"Fecha_Ocurrencia\"] = data[\"Fecha_Ocurrencia\"].apply(lambda x : pd.to_datetime(\"1899-12-30\") + datetime.timedelta(days=x))\n",
    "data[\"Fecha_Pago\"] = data[\"Fecha_Pago\"].apply(lambda x : pd.to_datetime(\"1899-12-30\") + datetime.timedelta(days=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = data.groupby([\"Fecha_Pago\"])[\"Importe USD\"].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fecha_inicial = aux[\"Fecha_Pago\"].min()\n",
    "fecha_final = aux[\"Fecha_Pago\"].max()\n",
    "\n",
    "rango_fechas = pd.date_range(fecha_inicial, fecha_final, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si una fecha no tiene registro, se agrega con importe 0\n",
    "aux = aux.set_index(\"Fecha_Pago\")\n",
    "aux = aux.reindex(rango_fechas, fill_value = 0)\n",
    "aux = aux.reset_index(names = [\"Fecha_Pago\", \"Importe USD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aux[[\"Fecha_Pago\", \"Importe USD\"]]\n",
    "y = aux[[\"Importe USD\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"anno\"] = X[\"Fecha_Pago\"].apply(lambda x : x.year)\n",
    "X[\"Mes\"] = X[\"Fecha_Pago\"].apply(lambda x : x.month)\n",
    "X[\"Dia\"] = X[\"Fecha_Pago\"].apply(lambda x : x.day)\n",
    "X[\"Dia_Semana\"] = X[\"Fecha_Pago\"].apply(lambda x : x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora cada uno de estos valores los transformaremos a un encoding cíclico.\n",
    "X[\"Dia_Semana_sin\"] = X[\"Dia_Semana\"].apply(lambda x : np.sin(2*np.pi*x/7))\n",
    "X[\"Dia_Semana_cos\"] = X[\"Dia_Semana\"].apply(lambda x : np.cos(2*np.pi*x/7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Mes_sin\"] = X[\"Mes\"].apply(lambda x : np.sin(2*np.pi*x/12))\n",
    "X[\"Mes_cos\"] = X[\"Mes\"].apply(lambda x : np.cos(2*np.pi*x/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Los días dependerán del mes, pues hay meses que tienen 30 días y otros 31, además de febrero que tiene 28 o 29.\n",
    "X[\"Dia_sin\"] = X.apply(lambda x : np.sin(2*np.pi*x[\"Dia\"]/x[\"Fecha_Pago\"].days_in_month), axis=1)\n",
    "X[\"Dia_cos\"] = X.apply(lambda x : np.cos(2*np.pi*x[\"Dia\"]/x[\"Fecha_Pago\"].days_in_month), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El año lo estandarizamos, pero comenzando desde 0.\n",
    "X[\"anno\"] = (X[\"anno\"] - X[\"anno\"].min()) / (X[\"anno\"].max() - X[\"anno\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"anno\", \"Dia_Semana_sin\", \"Dia_Semana_cos\", \"Mes_sin\", \"Mes_cos\", \"Dia_sin\", \"Dia_cos\", \"Importe USD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizamos los valores de y\n",
    "y_mean = y.values.mean()\n",
    "y_std = y.values.std()\n",
    "y = (y - y_mean) / y_std\n",
    "X[\"Importe USD\"] = (X[\"Importe USD\"] - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el X_train e y_train utilizando secuencias de un largo fijo\n",
    "sequence_length = 30\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(sequence_length, int(len(X) * 0.9)):\n",
    "    X_train.append(X.iloc[i - sequence_length : i][columns].values)\n",
    "    y_train.append(y.iloc[i])\n",
    "\n",
    "for i in range(int(len(X) * 0.9), len(X)):\n",
    "    X_test.append(X.iloc[i - sequence_length : i][columns].values)\n",
    "    y_test.append(y.iloc[i])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4560, 30, 8), (4560, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 30\n",
    "n_features = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_lstm, sequence_length = 30):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(n_lstm, input_shape=(n_steps, n_features)))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.20694094896316528\n",
      "Iteration 19 - Loss: 0.212728813290596\n",
      "Epoch 1/1000\n",
      "143/143 [==============================] - 3s 11ms/step - loss: 1.0968 - val_loss: 0.2173\n",
      "Epoch 2/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0949 - val_loss: 0.2159\n",
      "Epoch 3/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0907 - val_loss: 0.2140\n",
      "Epoch 4/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0889 - val_loss: 0.2166\n",
      "Epoch 5/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0897 - val_loss: 0.2217\n",
      "Epoch 6/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0859 - val_loss: 0.2172\n",
      "Epoch 7/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0857 - val_loss: 0.2207\n",
      "Epoch 8/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0833 - val_loss: 0.2271\n",
      "Epoch 9/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0826 - val_loss: 0.2125\n",
      "Epoch 10/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0793 - val_loss: 0.2363\n",
      "Epoch 11/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0747 - val_loss: 0.2219\n",
      "Epoch 12/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0715 - val_loss: 0.2166\n",
      "Epoch 13/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0832 - val_loss: 0.2274\n",
      "Epoch 14/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0694 - val_loss: 0.2353\n",
      "Epoch 15/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0628 - val_loss: 0.2436\n",
      "Epoch 16/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0564 - val_loss: 0.2554\n",
      "Epoch 17/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0610 - val_loss: 0.2574\n",
      "Epoch 18/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0587 - val_loss: 0.2889\n",
      "Epoch 19/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0573 - val_loss: 0.2764\n",
      "Epoch 20/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0435 - val_loss: 0.3454\n",
      "Epoch 21/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0336 - val_loss: 0.3522\n",
      "Epoch 22/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0196 - val_loss: 0.2398\n",
      "Epoch 23/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0334 - val_loss: 0.3989\n",
      "Epoch 24/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0189 - val_loss: 0.3680\n",
      "Epoch 25/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0085 - val_loss: 0.3745\n",
      "Epoch 26/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0107 - val_loss: 0.4756\n",
      "Epoch 27/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.9797 - val_loss: 0.2380\n",
      "Epoch 28/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 1.0685 - val_loss: 0.3726\n",
      "Epoch 29/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9739 - val_loss: 0.3642\n",
      "Epoch 30/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 1.0352 - val_loss: 0.3616\n",
      "Epoch 31/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.9708 - val_loss: 0.5089\n",
      "Epoch 32/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9257 - val_loss: 0.4896\n",
      "Epoch 33/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9481 - val_loss: 0.5533\n",
      "Epoch 34/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9176 - val_loss: 0.6350\n",
      "Epoch 35/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8978 - val_loss: 0.6018\n",
      "Epoch 36/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8713 - val_loss: 1.0058\n",
      "Epoch 37/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8581 - val_loss: 0.9195\n",
      "Epoch 38/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.7911 - val_loss: 1.3400\n",
      "Epoch 39/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.8468 - val_loss: 1.5698\n",
      "Epoch 40/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8726 - val_loss: 1.1683\n",
      "Epoch 41/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.7690 - val_loss: 1.2138\n",
      "Epoch 42/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.9701 - val_loss: 0.9867\n",
      "Epoch 43/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.7907 - val_loss: 1.5096\n",
      "Epoch 44/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.7330 - val_loss: 1.3587\n",
      "Epoch 45/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.6905 - val_loss: 1.5560\n",
      "Epoch 46/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.6817 - val_loss: 0.5798\n",
      "Epoch 47/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.8753 - val_loss: 1.3567\n",
      "Epoch 48/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.6827 - val_loss: 1.6140\n",
      "Epoch 49/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.6210 - val_loss: 2.0007\n",
      "Epoch 50/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.6041 - val_loss: 2.1265\n",
      "Epoch 51/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5600 - val_loss: 2.4380\n",
      "Epoch 52/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5606 - val_loss: 2.3433\n",
      "Epoch 53/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.6030 - val_loss: 2.3703\n",
      "Epoch 54/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5554 - val_loss: 2.3765\n",
      "Epoch 55/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.5296 - val_loss: 3.1718\n",
      "Epoch 56/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.5420 - val_loss: 2.2635\n",
      "Epoch 57/1000\n",
      "143/143 [==============================] - 1s 6ms/step - loss: 0.7455 - val_loss: 2.8961\n",
      "Epoch 58/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.8007 - val_loss: 0.4780\n",
      "Epoch 59/1000\n",
      "143/143 [==============================] - 1s 7ms/step - loss: 0.7557 - val_loss: 1.1287\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2125\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    print(f\"BEST LOSS: {best_loss}\")\n",
    "    print(f\"Iteration {i} - Loss: {loss}\")\n",
    "\n",
    "    model = create_model(64)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 1000,\n",
    "        batch_size = 32,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 50, restore_best_weights = True)])\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_model = model\n",
    "        best_loss = loss\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_17 (LSTM)              (None, 64)                18688     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,753\n",
      "Trainable params: 18,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(f\"./Models/lstm_64_{best_loss}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_lstm_1, n_lstm_2, sequence_length = 30):\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(n_lstm_1, input_shape=(n_steps, n_features), return_sequences=True))\n",
    "    model.add(tf.keras.layers.LSTM(n_lstm_2))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.20665493607521057\n",
      "Iteration 19 - Loss: 0.21121704578399658\n",
      "Epoch 1/1000\n",
      "143/143 [==============================] - 6s 18ms/step - loss: 1.0952 - val_loss: 0.2245\n",
      "Epoch 2/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0911 - val_loss: 0.2191\n",
      "Epoch 3/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0882 - val_loss: 0.2216\n",
      "Epoch 4/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0889 - val_loss: 0.2124\n",
      "Epoch 5/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0882 - val_loss: 0.2210\n",
      "Epoch 6/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0870 - val_loss: 0.2258\n",
      "Epoch 7/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0880 - val_loss: 0.2162\n",
      "Epoch 8/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0846 - val_loss: 0.2213\n",
      "Epoch 9/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0848 - val_loss: 0.2229\n",
      "Epoch 10/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0875 - val_loss: 0.2347\n",
      "Epoch 11/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0817 - val_loss: 0.2322\n",
      "Epoch 12/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0791 - val_loss: 0.2124\n",
      "Epoch 13/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0808 - val_loss: 0.2424\n",
      "Epoch 14/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0803 - val_loss: 0.2338\n",
      "Epoch 15/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0815 - val_loss: 0.2445\n",
      "Epoch 16/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0751 - val_loss: 0.2334\n",
      "Epoch 17/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0673 - val_loss: 0.2306\n",
      "Epoch 18/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0526 - val_loss: 0.2282\n",
      "Epoch 19/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0770 - val_loss: 0.2222\n",
      "Epoch 20/1000\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 1.0576 - val_loss: 0.2741\n",
      "Epoch 21/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0541 - val_loss: 0.2225\n",
      "Epoch 22/1000\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 1.0577 - val_loss: 0.2649\n",
      "Epoch 23/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0484 - val_loss: 0.2456\n",
      "Epoch 24/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0328 - val_loss: 0.3286\n",
      "Epoch 25/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0125 - val_loss: 0.2192\n",
      "Epoch 26/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0162 - val_loss: 0.2616\n",
      "Epoch 27/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9802 - val_loss: 0.2864\n",
      "Epoch 28/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0153 - val_loss: 0.3458\n",
      "Epoch 29/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9565 - val_loss: 0.3172\n",
      "Epoch 30/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9572 - val_loss: 0.4285\n",
      "Epoch 31/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0318 - val_loss: 0.3733\n",
      "Epoch 32/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9933 - val_loss: 0.3591\n",
      "Epoch 33/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9233 - val_loss: 0.2259\n",
      "Epoch 34/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9079 - val_loss: 0.2394\n",
      "Epoch 35/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8737 - val_loss: 0.2263\n",
      "Epoch 36/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8711 - val_loss: 0.4506\n",
      "Epoch 37/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8360 - val_loss: 0.2701\n",
      "Epoch 38/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.7971 - val_loss: 0.2498\n",
      "Epoch 39/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.7557 - val_loss: 0.2862\n",
      "Epoch 40/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.7322 - val_loss: 0.3574\n",
      "Epoch 41/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9026 - val_loss: 0.2294\n",
      "Epoch 42/1000\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 0.7882 - val_loss: 0.5372\n",
      "Epoch 43/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8245 - val_loss: 0.2268\n",
      "Epoch 44/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.7086 - val_loss: 0.2472\n",
      "Epoch 45/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6733 - val_loss: 0.2332\n",
      "Epoch 46/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6633 - val_loss: 0.3694\n",
      "Epoch 47/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6315 - val_loss: 0.2740\n",
      "Epoch 48/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6231 - val_loss: 0.3828\n",
      "Epoch 49/1000\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 0.6234 - val_loss: 0.3248\n",
      "Epoch 50/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6395 - val_loss: 0.8312\n",
      "Epoch 51/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6309 - val_loss: 0.4965\n",
      "Epoch 52/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6190 - val_loss: 0.3707\n",
      "Epoch 53/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.5816 - val_loss: 0.6650\n",
      "Epoch 54/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.5266 - val_loss: 0.7130\n",
      "Epoch 55/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.5141 - val_loss: 0.5825\n",
      "Epoch 56/1000\n",
      "143/143 [==============================] - 1s 9ms/step - loss: 0.4992 - val_loss: 0.4588\n",
      "Epoch 57/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.4922 - val_loss: 0.4005\n",
      "Epoch 58/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.5101 - val_loss: 1.0207\n",
      "Epoch 59/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.5163 - val_loss: 0.6263\n",
      "Epoch 60/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.4652 - val_loss: 0.9097\n",
      "Epoch 61/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.4550 - val_loss: 0.6843\n",
      "Epoch 62/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.4502 - val_loss: 0.3218\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2124\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    print(f\"BEST LOSS: {best_loss}\")\n",
    "    print(f\"Iteration {i} - Loss: {loss}\")\n",
    "\n",
    "    model = create_model(64, 32)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 1000,\n",
    "        batch_size = 32,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 50, restore_best_weights = True)])\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_model = model\n",
    "        best_loss = loss\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_55 (LSTM)              (None, 30, 64)            18688     \n",
      "                                                                 \n",
      " lstm_56 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,137\n",
      "Trainable params: 31,137\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(f\"./Models/lstm_64_32_{best_loss}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_lstm_1, n_lstm_2, sequence_length = 30):\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps, n_features))\n",
    "\n",
    "    x = tf.keras.layers.LSTM(n_lstm_1, return_sequences=True)(inputs)\n",
    "    x = tf.keras.layers.LSTM(n_lstm_2)(x)\n",
    "    x = tf.keras.layers.Dense(64)(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.20673413574695587\n",
      "Iteration 19 - Loss: 0.2098938673734665\n",
      "Epoch 1/1000\n",
      "143/143 [==============================] - 6s 17ms/step - loss: 1.0964 - val_loss: 0.2138\n",
      "Epoch 2/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0923 - val_loss: 0.2314\n",
      "Epoch 3/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0915 - val_loss: 0.2255\n",
      "Epoch 4/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0887 - val_loss: 0.2152\n",
      "Epoch 5/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0891 - val_loss: 0.2164\n",
      "Epoch 6/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0884 - val_loss: 0.2194\n",
      "Epoch 7/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0872 - val_loss: 0.2223\n",
      "Epoch 8/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0868 - val_loss: 0.2314\n",
      "Epoch 9/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0841 - val_loss: 0.2184\n",
      "Epoch 10/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0845 - val_loss: 0.2145\n",
      "Epoch 11/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0789 - val_loss: 0.2400\n",
      "Epoch 12/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0779 - val_loss: 0.2154\n",
      "Epoch 13/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0718 - val_loss: 0.2094\n",
      "Epoch 14/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0867 - val_loss: 0.2290\n",
      "Epoch 15/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0775 - val_loss: 0.2235\n",
      "Epoch 16/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0725 - val_loss: 0.2328\n",
      "Epoch 17/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0651 - val_loss: 0.3155\n",
      "Epoch 18/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0132 - val_loss: 0.2168\n",
      "Epoch 19/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0859 - val_loss: 0.2161\n",
      "Epoch 20/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0846 - val_loss: 0.2282\n",
      "Epoch 21/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0765 - val_loss: 0.2170\n",
      "Epoch 22/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0753 - val_loss: 0.2219\n",
      "Epoch 23/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0583 - val_loss: 0.2276\n",
      "Epoch 24/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0665 - val_loss: 0.2076\n",
      "Epoch 25/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0871 - val_loss: 0.2439\n",
      "Epoch 26/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0846 - val_loss: 0.2743\n",
      "Epoch 27/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0824 - val_loss: 0.2216\n",
      "Epoch 28/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0814 - val_loss: 0.2274\n",
      "Epoch 29/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0804 - val_loss: 0.2297\n",
      "Epoch 30/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0755 - val_loss: 0.2153\n",
      "Epoch 31/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0628 - val_loss: 0.3207\n",
      "Epoch 32/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0700 - val_loss: 0.2214\n",
      "Epoch 33/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0726 - val_loss: 0.2907\n",
      "Epoch 34/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0573 - val_loss: 0.2148\n",
      "Epoch 35/1000\n",
      "143/143 [==============================] - 2s 11ms/step - loss: 1.0889 - val_loss: 0.2324\n",
      "Epoch 36/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0622 - val_loss: 0.2161\n",
      "Epoch 37/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9915 - val_loss: 0.3526\n",
      "Epoch 38/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0401 - val_loss: 0.6960\n",
      "Epoch 39/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9821 - val_loss: 0.3840\n",
      "Epoch 40/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8911 - val_loss: 0.2285\n",
      "Epoch 41/1000\n",
      "143/143 [==============================] - 2s 11ms/step - loss: 1.0832 - val_loss: 0.3107\n",
      "Epoch 42/1000\n",
      "143/143 [==============================] - 2s 10ms/step - loss: 1.0379 - val_loss: 0.2521\n",
      "Epoch 43/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0644 - val_loss: 0.3203\n",
      "Epoch 44/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0001 - val_loss: 0.3998\n",
      "Epoch 45/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0407 - val_loss: 0.2773\n",
      "Epoch 46/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8666 - val_loss: 0.2191\n",
      "Epoch 47/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9118 - val_loss: 0.2732\n",
      "Epoch 48/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0519 - val_loss: 0.2622\n",
      "Epoch 49/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8831 - val_loss: 0.4845\n",
      "Epoch 50/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6868 - val_loss: 0.2158\n",
      "Epoch 51/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9112 - val_loss: 0.2843\n",
      "Epoch 52/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6575 - val_loss: 0.2915\n",
      "Epoch 53/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.5797 - val_loss: 0.2863\n",
      "Epoch 54/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6661 - val_loss: 0.2897\n",
      "Epoch 55/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0800 - val_loss: 0.2562\n",
      "Epoch 56/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 1.0139 - val_loss: 0.2599\n",
      "Epoch 57/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8672 - val_loss: 0.2626\n",
      "Epoch 58/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6105 - val_loss: 0.3873\n",
      "Epoch 59/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.4824 - val_loss: 0.2153\n",
      "Epoch 60/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.8141 - val_loss: 1.0114\n",
      "Epoch 61/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.9623 - val_loss: 0.2885\n",
      "Epoch 62/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.4470 - val_loss: 0.3277\n",
      "Epoch 63/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.3585 - val_loss: 0.3208\n",
      "Epoch 64/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.3137 - val_loss: 0.3319\n",
      "Epoch 65/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.2922 - val_loss: 0.3082\n",
      "Epoch 66/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.2910 - val_loss: 0.2651\n",
      "Epoch 67/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.3253 - val_loss: 0.2747\n",
      "Epoch 68/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.3998 - val_loss: 0.9093\n",
      "Epoch 69/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.7761 - val_loss: 0.2593\n",
      "Epoch 70/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.6734 - val_loss: 0.3225\n",
      "Epoch 71/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.3032 - val_loss: 0.2914\n",
      "Epoch 72/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.2847 - val_loss: 0.2790\n",
      "Epoch 73/1000\n",
      "143/143 [==============================] - 1s 10ms/step - loss: 0.2817 - val_loss: 0.2934\n",
      "Epoch 74/1000\n",
      "143/143 [==============================] - 2s 10ms/step - loss: 0.2798 - val_loss: 0.2546\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2076\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    print(f\"BEST LOSS: {best_loss}\")\n",
    "    print(f\"Iteration {i} - Loss: {loss}\")\n",
    "\n",
    "    model = create_model(64, 32)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 1000,\n",
    "        batch_size = 32,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 50, restore_best_weights = True)])\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_model = model\n",
    "        best_loss = loss\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 30, 8)]           0         \n",
      "                                                                 \n",
      " lstm_32 (LSTM)              (None, 30, 64)            18688     \n",
      "                                                                 \n",
      " lstm_33 (LSTM)              (None, 32)                12416     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,281\n",
      "Trainable params: 33,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"./Models/lstm_64_32_64_{best_loss}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps, n_features))\n",
    "\n",
    "    #This is going to be a RNN simple cell\n",
    "\n",
    "    x = tf.keras.layers.SimpleRNN(64, return_sequences=True)(inputs)\n",
    "    x = tf.keras.layers.SimpleRNN(32)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.1221190094947815\n",
      "Iteration 19 - Loss: 0.20685075223445892\n",
      "Epoch 1/1000\n",
      "143/143 [==============================] - 12s 71ms/step - loss: 1.1256 - val_loss: 0.2046\n",
      "Epoch 2/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.1062 - val_loss: 0.2483\n",
      "Epoch 3/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.1048 - val_loss: 0.2195\n",
      "Epoch 4/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.1004 - val_loss: 0.2227\n",
      "Epoch 5/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0888 - val_loss: 0.2141\n",
      "Epoch 6/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0912 - val_loss: 0.2501\n",
      "Epoch 7/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0839 - val_loss: 0.3217\n",
      "Epoch 8/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.1068 - val_loss: 0.2206\n",
      "Epoch 9/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0817 - val_loss: 0.2368\n",
      "Epoch 10/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0861 - val_loss: 0.2321\n",
      "Epoch 11/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0826 - val_loss: 0.2118\n",
      "Epoch 12/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0880 - val_loss: 0.2115\n",
      "Epoch 13/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0797 - val_loss: 0.2173\n",
      "Epoch 14/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0772 - val_loss: 0.2304\n",
      "Epoch 15/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.1103 - val_loss: 0.2217\n",
      "Epoch 16/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0932 - val_loss: 0.2411\n",
      "Epoch 17/1000\n",
      "143/143 [==============================] - 9s 66ms/step - loss: 1.1030 - val_loss: 0.3144\n",
      "Epoch 18/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0980 - val_loss: 0.2161\n",
      "Epoch 19/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0950 - val_loss: 0.2105\n",
      "Epoch 20/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0882 - val_loss: 0.2196\n",
      "Epoch 21/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0698 - val_loss: 0.2120\n",
      "Epoch 22/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0782 - val_loss: 0.2116\n",
      "Epoch 23/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0784 - val_loss: 0.2193\n",
      "Epoch 24/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0771 - val_loss: 0.2092\n",
      "Epoch 25/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0633 - val_loss: 0.2095\n",
      "Epoch 26/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0450 - val_loss: 0.3090\n",
      "Epoch 27/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0369 - val_loss: 0.2086\n",
      "Epoch 28/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0521 - val_loss: 0.2372\n",
      "Epoch 29/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0726 - val_loss: 0.2068\n",
      "Epoch 30/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0764 - val_loss: 0.2113\n",
      "Epoch 31/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0700 - val_loss: 0.2248\n",
      "Epoch 32/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0764 - val_loss: 0.2189\n",
      "Epoch 33/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0477 - val_loss: 0.2243\n",
      "Epoch 34/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0595 - val_loss: 0.2448\n",
      "Epoch 35/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0332 - val_loss: 0.2251\n",
      "Epoch 36/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0066 - val_loss: 0.2112\n",
      "Epoch 37/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.9798 - val_loss: 0.2298\n",
      "Epoch 38/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.9412 - val_loss: 0.2196\n",
      "Epoch 39/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.9532 - val_loss: 0.2250\n",
      "Epoch 40/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0798 - val_loss: 0.2010\n",
      "Epoch 41/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0718 - val_loss: 0.2176\n",
      "Epoch 42/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0506 - val_loss: 0.2475\n",
      "Epoch 43/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0299 - val_loss: 0.2436\n",
      "Epoch 44/1000\n",
      "143/143 [==============================] - 10s 70ms/step - loss: 0.9890 - val_loss: 0.2146\n",
      "Epoch 45/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.9516 - val_loss: 0.2113\n",
      "Epoch 46/1000\n",
      "143/143 [==============================] - 10s 70ms/step - loss: 1.0606 - val_loss: 0.2529\n",
      "Epoch 47/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0559 - val_loss: 0.2210\n",
      "Epoch 48/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0170 - val_loss: 0.2250\n",
      "Epoch 49/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.9994 - val_loss: 0.2596\n",
      "Epoch 50/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.9017 - val_loss: 0.2122\n",
      "Epoch 51/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.9953 - val_loss: 0.2256\n",
      "Epoch 52/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.9492 - val_loss: 0.2139\n",
      "Epoch 53/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0671 - val_loss: 0.2238\n",
      "Epoch 54/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0564 - val_loss: 0.2132\n",
      "Epoch 55/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0302 - val_loss: 0.3198\n",
      "Epoch 56/1000\n",
      "143/143 [==============================] - 10s 70ms/step - loss: 1.1284 - val_loss: 0.2243\n",
      "Epoch 57/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0730 - val_loss: 0.2221\n",
      "Epoch 58/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0788 - val_loss: 0.2130\n",
      "Epoch 59/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0561 - val_loss: 0.2287\n",
      "Epoch 60/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0268 - val_loss: 0.2361\n",
      "Epoch 61/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0019 - val_loss: 0.2209\n",
      "Epoch 62/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.9973 - val_loss: 0.2221\n",
      "Epoch 63/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0273 - val_loss: 0.2177\n",
      "Epoch 64/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0238 - val_loss: 0.2189\n",
      "Epoch 65/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0581 - val_loss: 0.2011\n",
      "Epoch 66/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0356 - val_loss: 0.2083\n",
      "Epoch 67/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0708 - val_loss: 0.2144\n",
      "Epoch 68/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0431 - val_loss: 0.2261\n",
      "Epoch 69/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0341 - val_loss: 0.2060\n",
      "Epoch 70/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0282 - val_loss: 0.1878\n",
      "Epoch 71/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.9922 - val_loss: 0.2007\n",
      "Epoch 72/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.9175 - val_loss: 0.1783\n",
      "Epoch 73/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.8339 - val_loss: 0.2112\n",
      "Epoch 74/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.8049 - val_loss: 0.2232\n",
      "Epoch 75/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.7457 - val_loss: 0.1719\n",
      "Epoch 76/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.7350 - val_loss: 0.2183\n",
      "Epoch 77/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.7139 - val_loss: 0.2425\n",
      "Epoch 78/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6901 - val_loss: 0.2091\n",
      "Epoch 79/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.9551 - val_loss: 0.2281\n",
      "Epoch 80/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0962 - val_loss: 0.2304\n",
      "Epoch 81/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0633 - val_loss: 0.1657\n",
      "Epoch 82/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0389 - val_loss: 0.1679\n",
      "Epoch 83/1000\n",
      "143/143 [==============================] - 11s 80ms/step - loss: 1.0296 - val_loss: 0.1612\n",
      "Epoch 84/1000\n",
      "143/143 [==============================] - 11s 76ms/step - loss: 1.0237 - val_loss: 0.1899\n",
      "Epoch 85/1000\n",
      "143/143 [==============================] - 10s 73ms/step - loss: 1.0268 - val_loss: 0.1384\n",
      "Epoch 86/1000\n",
      "143/143 [==============================] - 11s 75ms/step - loss: 1.0051 - val_loss: 0.1481\n",
      "Epoch 87/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.9266 - val_loss: 0.2465\n",
      "Epoch 88/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0831 - val_loss: 0.2036\n",
      "Epoch 89/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0286 - val_loss: 0.2208\n",
      "Epoch 90/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.8347 - val_loss: 0.2223\n",
      "Epoch 91/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.7242 - val_loss: 0.2366\n",
      "Epoch 92/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6597 - val_loss: 0.1850\n",
      "Epoch 93/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6422 - val_loss: 0.2751\n",
      "Epoch 94/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6189 - val_loss: 0.3267\n",
      "Epoch 95/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.6160 - val_loss: 0.4104\n",
      "Epoch 96/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6041 - val_loss: 0.3650\n",
      "Epoch 97/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.5977 - val_loss: 0.4130\n",
      "Epoch 98/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.5843 - val_loss: 0.5601\n",
      "Epoch 99/1000\n",
      "143/143 [==============================] - 10s 70ms/step - loss: 0.6130 - val_loss: 0.4650\n",
      "Epoch 100/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.5733 - val_loss: 0.3779\n",
      "Epoch 101/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.5684 - val_loss: 0.5798\n",
      "Epoch 102/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.8335 - val_loss: 0.2193\n",
      "Epoch 103/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0609 - val_loss: 0.2283\n",
      "Epoch 104/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 1.0010 - val_loss: 0.2376\n",
      "Epoch 105/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0891 - val_loss: 0.2244\n",
      "Epoch 106/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0588 - val_loss: 0.2368\n",
      "Epoch 107/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0545 - val_loss: 0.2242\n",
      "Epoch 108/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0068 - val_loss: 0.2341\n",
      "Epoch 109/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.9356 - val_loss: 0.2845\n",
      "Epoch 110/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0664 - val_loss: 0.2130\n",
      "Epoch 111/1000\n",
      "143/143 [==============================] - 10s 70ms/step - loss: 0.9915 - val_loss: 0.2949\n",
      "Epoch 112/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 1.0115 - val_loss: 0.1703\n",
      "Epoch 113/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 1.0389 - val_loss: 0.2469\n",
      "Epoch 114/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.9895 - val_loss: 0.2176\n",
      "Epoch 115/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.8619 - val_loss: 0.2289\n",
      "Epoch 116/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.7775 - val_loss: 0.3103\n",
      "Epoch 117/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.7260 - val_loss: 0.4907\n",
      "Epoch 118/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6513 - val_loss: 0.4505\n",
      "Epoch 119/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.6253 - val_loss: 0.5205\n",
      "Epoch 120/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6172 - val_loss: 0.3600\n",
      "Epoch 121/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.6012 - val_loss: 0.3985\n",
      "Epoch 122/1000\n",
      "143/143 [==============================] - 10s 71ms/step - loss: 0.5941 - val_loss: 0.4514\n",
      "Epoch 123/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.5899 - val_loss: 0.3917\n",
      "Epoch 124/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.5883 - val_loss: 0.3952\n",
      "Epoch 125/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.5838 - val_loss: 0.3531\n",
      "Epoch 126/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.5834 - val_loss: 0.5186\n",
      "Epoch 127/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.5823 - val_loss: 0.3612\n",
      "Epoch 128/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.5668 - val_loss: 0.3668\n",
      "Epoch 129/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.5713 - val_loss: 0.3805\n",
      "Epoch 130/1000\n",
      "143/143 [==============================] - 11s 76ms/step - loss: 0.5634 - val_loss: 0.4011\n",
      "Epoch 131/1000\n",
      "143/143 [==============================] - 10s 68ms/step - loss: 0.5716 - val_loss: 0.3938\n",
      "Epoch 132/1000\n",
      "143/143 [==============================] - 10s 71ms/step - loss: 0.5613 - val_loss: 0.3503\n",
      "Epoch 133/1000\n",
      "143/143 [==============================] - 10s 67ms/step - loss: 0.5480 - val_loss: 0.4179\n",
      "Epoch 134/1000\n",
      "143/143 [==============================] - 10s 69ms/step - loss: 0.5423 - val_loss: 0.3346\n",
      "Epoch 135/1000\n",
      "143/143 [==============================] - 10s 71ms/step - loss: 0.5363 - val_loss: 0.4166\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.1384\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    print(f\"BEST LOSS: {best_loss}\")\n",
    "    print(f\"Iteration {i} - Loss: {loss}\")\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 1000,\n",
    "        batch_size = 32,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 50, restore_best_weights = True)])\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_model = model\n",
    "        best_loss = loss\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 30, 8)]           0         \n",
      "                                                                 \n",
      " simple_rnn_8 (SimpleRNN)    (None, 30, 64)            4672      \n",
      "                                                                 \n",
      " simple_rnn_9 (SimpleRNN)    (None, 32)                3104      \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(f\"./Models/rnn_64_32_64_{best_loss}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps, n_features))\n",
    "\n",
    "    x = tf.keras.layers.SimpleRNN(64, return_sequences=True)(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.21163050830364227\n",
      "Iteration 19 - Loss: 0.21296530961990356\n",
      "Epoch 1/1000\n",
      "143/143 [==============================] - 5s 31ms/step - loss: 1.1065 - val_loss: 0.2843\n",
      "Epoch 2/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.1033 - val_loss: 0.2124\n",
      "Epoch 3/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0938 - val_loss: 0.2404\n",
      "Epoch 4/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0944 - val_loss: 0.2691\n",
      "Epoch 5/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0964 - val_loss: 0.2417\n",
      "Epoch 6/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0928 - val_loss: 0.2296\n",
      "Epoch 7/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0921 - val_loss: 0.2154\n",
      "Epoch 8/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0939 - val_loss: 0.2365\n",
      "Epoch 9/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0931 - val_loss: 0.2134\n",
      "Epoch 10/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0922 - val_loss: 0.2199\n",
      "Epoch 11/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0864 - val_loss: 0.2255\n",
      "Epoch 12/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0918 - val_loss: 0.2190\n",
      "Epoch 13/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0889 - val_loss: 0.2363\n",
      "Epoch 14/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0889 - val_loss: 0.2418\n",
      "Epoch 15/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0848 - val_loss: 0.2291\n",
      "Epoch 16/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0869 - val_loss: 0.2254\n",
      "Epoch 17/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0873 - val_loss: 0.2514\n",
      "Epoch 18/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0900 - val_loss: 0.2588\n",
      "Epoch 19/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0853 - val_loss: 0.2255\n",
      "Epoch 20/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0874 - val_loss: 0.2280\n",
      "Epoch 21/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0841 - val_loss: 0.2149\n",
      "Epoch 22/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0850 - val_loss: 0.2208\n",
      "Epoch 23/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0860 - val_loss: 0.2159\n",
      "Epoch 24/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0850 - val_loss: 0.2127\n",
      "Epoch 25/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0835 - val_loss: 0.2155\n",
      "Epoch 26/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0819 - val_loss: 0.2432\n",
      "Epoch 27/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0812 - val_loss: 0.2184\n",
      "Epoch 28/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0843 - val_loss: 0.2134\n",
      "Epoch 29/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0842 - val_loss: 0.2207\n",
      "Epoch 30/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0827 - val_loss: 0.2169\n",
      "Epoch 31/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0844 - val_loss: 0.2201\n",
      "Epoch 32/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0840 - val_loss: 0.2339\n",
      "Epoch 33/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0758 - val_loss: 0.2248\n",
      "Epoch 34/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0817 - val_loss: 0.2228\n",
      "Epoch 35/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0754 - val_loss: 0.2253\n",
      "Epoch 36/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0788 - val_loss: 0.2657\n",
      "Epoch 37/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0743 - val_loss: 0.2452\n",
      "Epoch 38/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0674 - val_loss: 0.2676\n",
      "Epoch 39/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0772 - val_loss: 0.2254\n",
      "Epoch 40/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0635 - val_loss: 0.2235\n",
      "Epoch 41/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0717 - val_loss: 0.2484\n",
      "Epoch 42/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0555 - val_loss: 0.2189\n",
      "Epoch 43/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0435 - val_loss: 0.2224\n",
      "Epoch 44/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0639 - val_loss: 0.2236\n",
      "Epoch 45/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0672 - val_loss: 0.2536\n",
      "Epoch 46/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0579 - val_loss: 0.2240\n",
      "Epoch 47/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0455 - val_loss: 0.2182\n",
      "Epoch 48/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0692 - val_loss: 0.2233\n",
      "Epoch 49/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0881 - val_loss: 0.2169\n",
      "Epoch 50/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0835 - val_loss: 0.2245\n",
      "Epoch 51/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0793 - val_loss: 0.2614\n",
      "Epoch 52/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0853 - val_loss: 0.2375\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2124\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    print(f\"BEST LOSS: {best_loss}\")\n",
    "    print(f\"Iteration {i} - Loss: {loss}\")\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 1000,\n",
    "        batch_size = 32,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 50, restore_best_weights = True)])\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_model = model\n",
    "        best_loss = loss\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_52 (InputLayer)       [(None, 30, 8)]           0         \n",
      "                                                                 \n",
      " simple_rnn_50 (SimpleRNN)   (None, 30, 64)            4672      \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 30, 64)            4160      \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 30, 1)             65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,897\n",
      "Trainable params: 8,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(f\"./Models/rnn_64_64_{best_loss}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps, n_features))\n",
    "\n",
    "    x = tf.keras.layers.SimpleRNN(128, return_sequences=True)(inputs)\n",
    "    x = tf.keras.layers.SimpleRNN(64)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST LOSS: 0.21064460277557373\n",
      "Iteration 19 - Loss: 0.21632631123065948\n",
      "Epoch 1/1000\n",
      "143/143 [==============================] - 6s 31ms/step - loss: 1.1083 - val_loss: 0.2178\n",
      "Epoch 2/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0995 - val_loss: 0.2687\n",
      "Epoch 3/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0963 - val_loss: 0.2159\n",
      "Epoch 4/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0940 - val_loss: 0.2426\n",
      "Epoch 5/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0931 - val_loss: 0.2673\n",
      "Epoch 6/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0910 - val_loss: 0.2274\n",
      "Epoch 7/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0916 - val_loss: 0.2436\n",
      "Epoch 8/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0903 - val_loss: 0.2165\n",
      "Epoch 9/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0897 - val_loss: 0.2187\n",
      "Epoch 10/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0880 - val_loss: 0.2295\n",
      "Epoch 11/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0881 - val_loss: 0.2165\n",
      "Epoch 12/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0889 - val_loss: 0.2156\n",
      "Epoch 13/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0866 - val_loss: 0.2313\n",
      "Epoch 14/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0854 - val_loss: 0.2460\n",
      "Epoch 15/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0890 - val_loss: 0.2165\n",
      "Epoch 16/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0873 - val_loss: 0.2190\n",
      "Epoch 17/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0858 - val_loss: 0.2194\n",
      "Epoch 18/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0861 - val_loss: 0.2216\n",
      "Epoch 19/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0863 - val_loss: 0.2232\n",
      "Epoch 20/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0853 - val_loss: 0.2239\n",
      "Epoch 21/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0844 - val_loss: 0.2286\n",
      "Epoch 22/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0840 - val_loss: 0.2139\n",
      "Epoch 23/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0832 - val_loss: 0.2211\n",
      "Epoch 24/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0810 - val_loss: 0.2198\n",
      "Epoch 25/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0825 - val_loss: 0.2228\n",
      "Epoch 26/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0866 - val_loss: 0.2178\n",
      "Epoch 27/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0829 - val_loss: 0.2194\n",
      "Epoch 28/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0832 - val_loss: 0.2143\n",
      "Epoch 29/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0774 - val_loss: 0.2638\n",
      "Epoch 30/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0810 - val_loss: 0.2150\n",
      "Epoch 31/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0753 - val_loss: 0.2181\n",
      "Epoch 32/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0822 - val_loss: 0.2209\n",
      "Epoch 33/1000\n",
      "143/143 [==============================] - 4s 28ms/step - loss: 1.0730 - val_loss: 0.2155\n",
      "Epoch 34/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0778 - val_loss: 0.2189\n",
      "Epoch 35/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0828 - val_loss: 0.2141\n",
      "Epoch 36/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0839 - val_loss: 0.2216\n",
      "Epoch 37/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0683 - val_loss: 0.2145\n",
      "Epoch 38/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0805 - val_loss: 0.2467\n",
      "Epoch 39/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0713 - val_loss: 0.2207\n",
      "Epoch 40/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0728 - val_loss: 0.2155\n",
      "Epoch 41/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0610 - val_loss: 0.2162\n",
      "Epoch 42/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0731 - val_loss: 0.2212\n",
      "Epoch 43/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0475 - val_loss: 0.2250\n",
      "Epoch 44/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0399 - val_loss: 0.2282\n",
      "Epoch 45/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0320 - val_loss: 0.2247\n",
      "Epoch 46/1000\n",
      "143/143 [==============================] - 4s 31ms/step - loss: 1.0414 - val_loss: 0.2352\n",
      "Epoch 47/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0474 - val_loss: 0.2504\n",
      "Epoch 48/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0715 - val_loss: 0.2279\n",
      "Epoch 49/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0668 - val_loss: 0.2220\n",
      "Epoch 50/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0310 - val_loss: 0.2515\n",
      "Epoch 51/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0837 - val_loss: 0.2292\n",
      "Epoch 52/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0257 - val_loss: 0.2212\n",
      "Epoch 53/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.9740 - val_loss: 0.2369\n",
      "Epoch 54/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0008 - val_loss: 0.2283\n",
      "Epoch 55/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0693 - val_loss: 0.2390\n",
      "Epoch 56/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0209 - val_loss: 0.2600\n",
      "Epoch 57/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.9920 - val_loss: 0.2550\n",
      "Epoch 58/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.9801 - val_loss: 0.2279\n",
      "Epoch 59/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.9781 - val_loss: 0.2553\n",
      "Epoch 60/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.9529 - val_loss: 0.3308\n",
      "Epoch 61/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.9210 - val_loss: 0.2217\n",
      "Epoch 62/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.9783 - val_loss: 0.3557\n",
      "Epoch 63/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.9008 - val_loss: 0.2258\n",
      "Epoch 64/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.8868 - val_loss: 0.4462\n",
      "Epoch 65/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.1195 - val_loss: 0.2435\n",
      "Epoch 66/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 1.0878 - val_loss: 0.2356\n",
      "Epoch 67/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 1.0356 - val_loss: 0.2460\n",
      "Epoch 68/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.9865 - val_loss: 0.2291\n",
      "Epoch 69/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.9546 - val_loss: 0.2411\n",
      "Epoch 70/1000\n",
      "143/143 [==============================] - 4s 29ms/step - loss: 0.9149 - val_loss: 0.2608\n",
      "Epoch 71/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.8910 - val_loss: 0.2428\n",
      "Epoch 72/1000\n",
      "143/143 [==============================] - 4s 30ms/step - loss: 0.8762 - val_loss: 0.2603\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.2139\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_loss = float(\"inf\")\n",
    "loss = float(\"inf\")\n",
    "\n",
    "for i in range(20):\n",
    "\n",
    "    print(f\"BEST LOSS: {best_loss}\")\n",
    "    print(f\"Iteration {i} - Loss: {loss}\")\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data = (X_test, y_test),\n",
    "        epochs = 1000,\n",
    "        batch_size = 32,\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 50, restore_best_weights = True)])\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "    if loss < best_loss:\n",
    "        best_model = model\n",
    "        best_loss = loss\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_61\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_62 (InputLayer)       [(None, 30, 8)]           0         \n",
      "                                                                 \n",
      " simple_rnn_60 (SimpleRNN)   (None, 30, 64)            4672      \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 30, 64)            4160      \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 30, 1)             65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,897\n",
      "Trainable params: 8,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(f\"./Models/rnn_128_64_128_{best_loss}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps, n_features))\n",
    "\n",
    "    #This is going to be a RNN simple cell\n",
    "\n",
    "    x = tf.keras.layers.SimpleRNN(64, return_sequences=True)(inputs)\n",
    "    x = tf.keras.layers.SimpleRNN(32)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss = tf.keras.losses.MeanSquaredError(),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
